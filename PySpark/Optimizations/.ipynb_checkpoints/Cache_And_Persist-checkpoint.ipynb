{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Cache and Persist \n",
    "\n",
    "Spark `cache` and `persist` are optimization techniques for iterative and interactive Spark applications to improve\n",
    "the performance of the jobs or applications. In this article, you will learn What is Spark Caching and Persistence,\n",
    "the difference between `cache()` and `persist()` methods and how to use these two with RDD, DataFrame,\n",
    "and Dataset with Python examples.\n",
    "Though Spark provides computation 100 x times faster than traditional Map Reduce jobs,\n",
    "if you have not designed the jobs to reuse the repeating computations you will see degrade\n",
    "in performance when you are dealing with billions or trillions of data. Hence, we may need to look at\n",
    "the stages and use optimization techniques as one of the ways to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `cache()` and `persist()` methods, Spark provides an optimization mechanism to store the intermediate computation of \n",
    "an RDD, DataFrame, and Dataset so they can be reused in subsequent actions(reusing the RDD, Dataframe, \n",
    "and Dataset computation result’s).\n",
    "\n",
    "Both caching and persisting are used to save the Spark RDD, Dataframe, and Dataset’s. But, the difference is, RDD \n",
    "`cache()` method default saves it to memory (`MEMORY_ONLY`) whereas `persist()` method is used to store it \n",
    "to the user-defined storage level.\n",
    "\n",
    "When you persist a dataset, each node stores its partitioned data in memory and reuses them in other actions on \n",
    "that dataset. And Spark’s persisted data on nodes are fault-tolerant meaning if any partition of a Dataset is lost, \n",
    "it will automatically be recomputed using the original transformations that created it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Advantages for Caching and Persistence\n",
    "\n",
    "Cost efficient – Spark computations are very expensive hence reusing the computations are used to save cost.\n",
    "\n",
    "Time efficient – Reusing the repeated computations saves lots of time.\n",
    "\n",
    "Execution time – Saves execution time of the job and we can perform more jobs on the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cache Syntax and Example\n",
    "\n",
    "Spark DataFrame or Dataset caching by default saves it to storage level `MEMORY_AND_DISK` \n",
    "because recomputing the in-memory columnar representation of the underlying table is expensive. \n",
    "Note that this is different from the default cache level of `RDD.cache()` which is `MEMORY_ONLY`.\n",
    "\n",
    "## Syntax\n",
    "\n",
    "Spark `cache()` method in Dataset class internally calls `persist()` method which in turn uses \n",
    "`sparkSession.sharedState.cacheManager.cacheQuery` to cache the result set of DataFrame or Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "#\n",
    "# spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "# df = spark.createDataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)], ['dogs', 'cats'])\n",
    "#\n",
    "# # This makes the operation a lot faster\n",
    "# df = df.cache()\n",
    "#\n",
    "# df.groupBy('dogs').min().show()\n",
    "# df.groupBy('dogs').min().show()\n",
    "#\n",
    "# # To uncache:\n",
    "# df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Persist Syntax and Examples\n",
    "\n",
    "Spark `persist` has two signature first signature doesn't take any argument which by default saves it to \n",
    "`MEMORY_AND_DISK` storage level and the second signature which takes StorageLevel as an \n",
    "argument to store it to different storage levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "#\n",
    "# spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "# df = spark.createDataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)], ['dogs', 'cats'])\n",
    "#\n",
    "# # This makes the operation a lot faster\n",
    "# df = df.persist()\n",
    "#\n",
    "# df.groupBy('dogs').min().show()\n",
    "# df.groupBy('dogs').min().show()\n",
    "#\n",
    "# # To uncache:\n",
    "# df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the second signature you can save DataFrame/Dataset to One of the storage levels `MEMORY_ONLY`, `MEMORY_AND_DISK`, \n",
    "`MEMORY_ONLY_SER`, `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`, `MEMORY_AND_DISK_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "#\n",
    "# spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "# df = spark.createDataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)], ['dogs', 'cats'])\n",
    "#\n",
    "# # This makes the operation a lot faster\n",
    "# df = df.persist(StorageLevel.MEMORY_ONLY)\n",
    "#\n",
    "# df.groupBy('dogs').min().show()\n",
    "# df.groupBy('dogs').min().show()\n",
    "#\n",
    "# # To uncache:\n",
    "# df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Persistence storage Levels\n",
    "\n",
    "'''\n",
    "All different storage level Spark supports are available at `org.apache.spark.storage.StorageLevel` class. \n",
    "The storage level specifies how and where to persist or cache a Spark DataFrame and Dataset.\n",
    "\n",
    "`MEMORY_ONLY` – This is the default behavior of the RDD `cache()` method and stores the RDD or DataFrame as deserialized \n",
    "objects to JVM memory. When there is no enough memory available it will not save DataFrame of some partitions and these \n",
    "will be re-computed as and when required. This takes more memory. but unlike RDD, this would be slower than \n",
    "`MEMORY_AND_DISK` level as it recomputes the unsaved partitions and recomputing the in-memory columnar representation \n",
    "of the underlying table is expensive\n",
    "\n",
    "`MEMORY_ONLY_SER` – This is the same as `MEMORY_ONLY` but the difference being it stores RDD as serialized objects to JVM \n",
    "memory. It takes lesser memory (space-efficient) then `MEMORY_ONLY` as it saves objects as serialized and \n",
    "takes an additional few more CPU cycles in order to deserialize.\n",
    "\n",
    "`MEMORY_ONLY_2` – Same as `MEMORY_ONLY` storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "`MEMORY_ONLY_SER_2` – Same as `MEMORY_ONLY_SER` storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "`MEMORY_AND_DISK` – This is the default behavior of the DataFrame or Dataset. In this Storage Level, The DataFrame will\n",
    " be stored in JVM memory as a deserialized object. When required storage is greater than available memory, it stores \n",
    " some of the excess partitions into the disk and reads the data from the disk when required. It is slower as \n",
    " there is I/O involved.\n",
    "\n",
    "`MEMORY_AND_DISK_SER` – This is the same as `MEMORY_AND_DISK` storage level difference being it serializes the DataFrame \n",
    "objects in memory and on disk when space is not available.\n",
    "\n",
    "`MEMORY_AND_DISK_2` – Same as `MEMORY_AND_DISK` storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "`MEMORY_AND_DISK_SER_2` – Same as `MEMORY_AND_DISK_SER` storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "`DISK_ONLY` – In this storage level, DataFrame is stored only on disk and the CPU computation time \n",
    "is high as I/O is involved.\n",
    "\n",
    "`DISK_ONLY_2` – Same as DISK_ONLY storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "Below are the table representation of the Storage level, Go through the impact of space, cpu and performance \n",
    "choose the one that best fits for you.\n",
    "\n",
    "| Storage Level | Space used | CPU time | In memory | On-disk | Serialized | Recompute some partitions |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "MEMORY_ONLY | High | Low | Y | N | N | Y    \n",
    "MEMORY_ONLY_SER | Low | High | Y | N | Y | Y\n",
    "MEMORY_AND_DISK | High | Medium | Some | Some | Some | N\n",
    "MEMORY_AND_DISK_SER | Low | High | Some | Some | Y | N\n",
    "DISK_ONLY | Low | High | N | Y | Y | N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Points to note on Persistence\n",
    "\n",
    "Spark automatically monitors every `persist()` and `cache()` calls you make and it checks usage on \n",
    "each node and drops persisted data if not used or using least-recently-used (LRU) algorithm. \n",
    "\n",
    "As discussed in one of the above section you can also manually remove using `unpersist()` method.\n",
    "\n",
    "Spark caching and persistence is just one of the optimization techniques to improve the performance of Spark jobs.\n",
    "\n",
    "For RDD `cache()` default storage level is `MEMORY_ONLY` but, for DataFrame and Dataset, default is `MEMORY_AND_DISK`\n",
    "\n",
    "On Spark UI, the Storage tab shows where partitions exist in memory or disk across the cluster.\n",
    "\n",
    "Dataset `cache()` is an alias for `persist(StorageLevel.MEMORY_AND_DISK)`\n",
    "\n",
    "Caching of Spark DataFrame or Dataset is a lazy operation, meaning a DataFrame will not be cached until \n",
    "you trigger an action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
