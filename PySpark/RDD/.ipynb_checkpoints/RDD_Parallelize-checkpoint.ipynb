{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# RDD parallelize\n",
    "PySpark `parallelize()` is a function in SparkContext and is used to create an RDD from a list collection.\n",
    " In this article, I will explain the usage of parallelize to create RDD and how to create an empty\n",
    " RDD with PySpark example.\n",
    "\n",
    "Before we start let me explain what is RDD, Resilient Distributed Datasets (RDD) is a fundamental\n",
    "data structure of PySpark, It is an immutable distributed collection of objects. Each dataset in RDD is\n",
    "divided into logical partitions, which may be computed on different nodes of the cluster.\n",
    "\n",
    "PySpark Parallelizing an existing collection in your driver program.\n",
    "Below is an example of how to create an RDD using a parallelize method from Sparkcontext.\n",
    "\n",
    "`parkContext.parallelize([1,2,3,4,5,6,7,8,9,10])` creates an RDD with a list of Integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 8\n",
      "Action: First element: 1\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "sparkContext=spark.sparkContext\n",
    "\n",
    "rdd=sparkContext.parallelize([1,2,3,4,5])\n",
    "rddCollect = rdd.collect()\n",
    "print(\"Number of Partitions: \"+str(rdd.getNumPartitions()))\n",
    "print(\"Action: First element: \"+str(rdd.first()))\n",
    "print(rddCollect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parallelize()` function also has another signature which additionally takes integer argument to specifies the number \n",
    "of partitions. Partitions are basic units of parallelism in PySpark.\n",
    "\n",
    "Remember, RDDs in PySpark are a collection of partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create empty RDD by using sparkContext.parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is Empty RDD : True\n"
     ]
    }
   ],
   "source": [
    "emptyRDD = sparkContext.emptyRDD()\n",
    "emptyRDD2 = rdd=sparkContext.parallelize([])\n",
    "\n",
    "print(\"is Empty RDD : \"+str(emptyRDD2.isEmpty()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
