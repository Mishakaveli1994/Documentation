{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# repartition() vs coalesce()\n",
    "\n",
    "`repartition()` is used to\n",
    "increase or decrease the RDD/DataFrame partitions whereas the PySpark `coalesce()` is used to only decrease the\n",
    "number of partitions in an efficient way.\n",
    "\n",
    "One important point to note is, PySpark `repartition()` and coalesce() are very expensive operations as they shuffle\n",
    "the data across many partitions hence try to minimize using these as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From local[5]8\n",
      "parallelize : 6\n",
      "TextFile : 10\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "rdd = spark.sparkContext.parallelize((0, 20))\n",
    "print(\"From local[5]\" + str(rdd.getNumPartitions()))\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize((0, 25), 6)\n",
    "print(\"parallelize : \" + str(rdd1.getNumPartitions()))\n",
    "\n",
    "rddFromFile = spark.sparkContext.textFile(\"../Example_Sources/test.txt\", 10)\n",
    "print(\"TextFile : \" + str(rddFromFile.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rdd1.saveAsTextFile(\"/tmp/partition\")\n",
    "\n",
    "//Writes 6 part files, one for each partition\n",
    "\n",
    "Partition 1 : 0 1 2\n",
    "\n",
    "Partition 2 : 3 4 5\n",
    "\n",
    "Partition 3 : 6 7 8 9\n",
    "\n",
    "Partition 4 : 10 11 12\n",
    "\n",
    "Partition 5 : 13 14 15\n",
    "\n",
    "Partition 6 : 16 17 18 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Repartition\n",
    "Spark RDD `repartition()` method is used to increase or decrease the partitions. The below example decreases the \n",
    "partitions from 10 to 4 by moving data from all partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition size : 4\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd1.repartition(4)\n",
    "print(\"Repartition size : \" + str(rdd2.getNumPartitions()))\n",
    "# rdd2.saveAsTextFile(\"/tmp/re-partition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields output Repartition size : 4 and the repartition re-distributes the data(as shown below) from all \n",
    "partitions which is full shuffle leading to very expensive operation when dealing with billions and trillions of data.\n",
    "\n",
    "Partition 1 : 1 6 10 15 19\n",
    "\n",
    "Partition 2 : 2 3 7 11 16\n",
    "\n",
    "Partition 3 : 4 8 12 13 17\n",
    "\n",
    "Partition 4 : 0 5 9 14 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Coalesce\n",
    "Spark RDD `coalesce()` is used only to reduce the number of partitions. This is optimized or improved version of \n",
    "`repartition()` where the movement of the data across the partitions is lower using coalesce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition size : 4\n"
     ]
    }
   ],
   "source": [
    "rdd3 = rdd1.coalesce(4)\n",
    "print(\"Repartition size : \" + str(rdd3.getNumPartitions()))\n",
    "# rdd3.saveAsTextFile(\"/tmp/coalesce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compared the below output with section 1, you will notice partition 3 has been moved to 2 and Partition 6 \n",
    "has moved to 5, resulting data movement from just 2 partitions.\n",
    "\n",
    "Partition 1 : 0 1 2\n",
    "\n",
    "Partition 2 : 3 4 5 6 7 8 9\n",
    "\n",
    "Partition 4 : 10 11 12 \n",
    "\n",
    "Partition 5 : 13 14 15 16 17 18 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On DataFrame\n",
    "Like RDD, you canâ€™t specify the partition/parallelism while creating DataFrame. DataFrame by default internally uses \n",
    "the methods specified in Section 1 to determine the default partition and splits the data for parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com') \\\n",
    "    .master(\"local[5]\").getOrCreate()\n",
    "\n",
    "df = spark.range(0, 20)\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "# df.write.mode(\"overwrite\").csv(\"c:/tmp/partition.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example creates 5 partitions as specified in master(\"local[5]\") and the data is distributed \n",
    "across all these 5 partitions.\n",
    "\n",
    "Partition 1 : 0 1 2 3\n",
    "\n",
    "Partition 2 : 4 5 6 7\n",
    "\n",
    "Partition 3 : 8 9 10 11\n",
    "\n",
    "Partition 4 : 12 13 14 15\n",
    "\n",
    "Partition 5 : 16 17 18 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Repartition\n",
    "Similar to RDD, the PySpark DataFrame `repartition()` method is used to increase or decrease the partitions. \n",
    "The below example increases the partitions from 5 to 6 by moving data from all partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "df2 = df.repartition(6)\n",
    "print(df2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just increasing 1 partition results data movements from all partitions.\n",
    "\n",
    "Partition 1 : 14 1 5\n",
    "\n",
    "Partition 2 : 4 16 15\n",
    "\n",
    "Partition 3 : 8 3 18\n",
    "\n",
    "Partition 4 : 12 2 19\n",
    "\n",
    "Partition 5 : 6 17 7 0\n",
    "\n",
    "Partition 6 : 9 10 11 13\n",
    "\n",
    "And, even decreasing the partitions also results in moving data from all partitions, hence when you wanted to decrease \n",
    "the partition recommendation is to use `coalesce()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Coalesce\n",
    "Spark DataFrame `coalesce()` is used only to decrease the number of partitions. This is an optimized or improved\n",
    "version of `repartition()` where the movement of the data across the partitions is fewer using coalesce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "df3 = df.coalesce(2)\n",
    "print(df3.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields output 2 and the resultant partition looks like\n",
    "\n",
    "Partition 1 : 0 1 2 3 8 9 10 11\n",
    "\n",
    "Partition 2 : 4 5 6 7 12 13 14 15 16 17 18 19\n",
    "\n",
    "Since we are reducing 5 to 2 partitions, the data movement happens only from 3 partitions and it \n",
    "moves to remain 2 partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Shuffle Partition\n",
    "Calling `groupBy()`, `union()`, `join()` and similar functions on DataFrame results in shuffling data between multiple \n",
    "executors and even machines and finally repartitions data into 200 partitions by default. PySpark default defines \n",
    "shuffling partition to 200 using `spark.sql.shuffle.partitions` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "df4 = df.groupBy(\"id\").count()\n",
    "print(df4.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post shuffle operations, you can change the partitions either using `coalesce()` or `repartition()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
